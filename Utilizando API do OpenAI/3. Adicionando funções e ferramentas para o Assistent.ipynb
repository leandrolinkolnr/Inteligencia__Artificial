{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6f20c4",
   "metadata": {},
   "source": [
    "# Adicionando funções e ferramentas para o Assistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ead47b",
   "metadata": {},
   "source": [
    "# Definindo função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63039a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c94f2",
   "metadata": {},
   "source": [
    "# Definindo tools\n",
    "\n",
    "#### Define as ferramentas que o modelo pode usar para gerar as respostas. Uma função que pode ser usada pelo modelo.\n",
    "#### Formato específico de dicionários aninhados dentro de uma lista, para que o modelo entenda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bc8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51d893",
   "metadata": {},
   "source": [
    "# Mapenado funções\n",
    "\n",
    "#### Mapeia nomes de funções aos objetos de função, para ser mais facil de chamar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a04a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcoes_disponiveis = {\n",
    "        \"obter_temperatura_atual\": obter_temperatura_atual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396e02b",
   "metadata": {},
   "source": [
    "# Enviando mensagem e capturando resposta\n",
    "\n",
    "#### O parâmetro tools  informa ao modelo quais ferramentas (funções) estão disponíveis para ele utilizar.  \n",
    "\n",
    "####  Tool_choice=\"auto\" = indica ao modelo qual ferramenta utilizar. Quando \"auto\", ele decide automaticamente quando e como usa-las.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb7301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Qual é a temperatura em São Paulo e Porto Alegre?\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e475",
   "metadata": {},
   "source": [
    "## Como fica a variavel resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e967b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f8150",
   "metadata": {},
   "source": [
    "#### Podemos ver que ainda não há resposta... Mas ele tem algo pra nos informar, ele precisa de algo pra nos dar a resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06066a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9PvOpQGkQJmjCAgqheiMxHX1rbQIU', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_duaszJWi9ZZ6d14GZJcSqiV3', function=Function(arguments='{\"local\": \"São Paulo\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_EtY9TgZ7HqLTYJoTJ0QtmJaZ', function=Function(arguments='{\"local\": \"Porto Alegre\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function')]))], created=1715965871, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=69, prompt_tokens=90, total_tokens=159))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b5609",
   "metadata": {},
   "source": [
    "### Pegando realmente so o que nos interessa da resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ab2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_duaszJWi9ZZ6d14GZJcSqiV3', function=Function(arguments='{\"local\": \"São Paulo\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_EtY9TgZ7HqLTYJoTJ0QtmJaZ', function=Function(arguments='{\"local\": \"Porto Alegre\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem_resp = resposta.choices[0].message\n",
    "mensagem_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3366b",
   "metadata": {},
   "source": [
    "# Aplicando tool_calls\n",
    "\n",
    "#### O `.tool_calls` extrai e lista chamadas de ferramentas que o modelo solicita durante a geração da resposta. A finalidade é identificar quais funções precisam ser executadas para fornecer informações  para a resposta do modelo.\n",
    "\n",
    "Apresenta o id, a Function, seu nome e  seus argumentos. Podemos ver abaixo que houve 2 chamadas.\n",
    "\n",
    "### Então para nos dar a resposta, nos precisamos rodar essas funções com os parametros identificados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e8f2cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_duaszJWi9ZZ6d14GZJcSqiV3', function=Function(arguments='{\"local\": \"São Paulo\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_EtY9TgZ7HqLTYJoTJ0QtmJaZ', function=Function(arguments='{\"local\": \"Porto Alegre\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls = mensagem_resp.tool_calls\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1473ce",
   "metadata": {},
   "source": [
    "### E para cada chamada dessa, vamos chamar a função para o modelo conseguir as respostas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0a7bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A temperatura em São Paulo é de 32°C e em Porto Alegre é de 25°C.\n"
     ]
    }
   ],
   "source": [
    "if tool_calls:  # Se houver chamada de ferramentas\n",
    "    mensagens.append(mensagem_resp)   # Adicione a resposta na conversa\n",
    "    for tool_call in tool_calls:      # Para cada chamada\n",
    "        function_name = tool_call.function.name  # Extraia o nome da função\n",
    "        function_to_call = funcoes_disponiveis[function_name] # Obtem a função, atraves de 'funcoes_disponiveis' e seu nome\n",
    "        function_args = json.loads(tool_call.function.arguments)  # Decodifica os argumentos da função do JSON\n",
    "        function_response = function_to_call( # Chama a função e passa os argumentos extraidos\n",
    "            local=function_args.get(\"local\"),\n",
    "            unidade=function_args.get(\"unidade\"),\n",
    "        )\n",
    "        mensagens.append(      # Adiciona a resposta da função a conversa\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,    # O id\n",
    "                \"role\": \"tool\",     # Role é a ferramenta\n",
    "                \"name\": function_name,     # O nome\n",
    "                \"content\": function_response,   # E a resposta fornecida pelo modelo\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    segunda_resposta = client.chat.completions.create(    # Capturando a resposta final do modelo, depois de utilizar a funcao\n",
    "        model=\"gpt-3.5-turbo-0125\",   \n",
    "        messages=mensagens,\n",
    "    )\n",
    "\n",
    "mensagem_resp = segunda_resposta.choices[0].message   # Imprimindo a resposta final do modelo\n",
    "print(mensagem_resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7c602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
